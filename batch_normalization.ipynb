{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook compares the performance of a neural network that uses Batch Normalization to one that does not."
      ],
      "metadata": {
        "id": "EMU-REPEJdLn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KJdzTRSfCX4",
        "outputId": "b6280834-f277-498a-faf0-bce76282a227"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training without Batch Normalization:\n",
            "[1,   100] loss: 1.166\n",
            "[1,   200] loss: 0.446\n",
            "[1,   300] loss: 0.361\n",
            "[1,   400] loss: 0.323\n",
            "[1,   500] loss: 0.288\n",
            "[1,   600] loss: 0.267\n",
            "[1,   700] loss: 0.231\n",
            "[1,   800] loss: 0.246\n",
            "[1,   900] loss: 0.210\n",
            "[2,   100] loss: 0.200\n",
            "[2,   200] loss: 0.174\n",
            "[2,   300] loss: 0.171\n",
            "[2,   400] loss: 0.157\n",
            "[2,   500] loss: 0.155\n",
            "[2,   600] loss: 0.145\n",
            "[2,   700] loss: 0.135\n",
            "[2,   800] loss: 0.146\n",
            "[2,   900] loss: 0.145\n",
            "[3,   100] loss: 0.132\n",
            "[3,   200] loss: 0.127\n",
            "[3,   300] loss: 0.109\n",
            "[3,   400] loss: 0.117\n",
            "[3,   500] loss: 0.110\n",
            "[3,   600] loss: 0.106\n",
            "[3,   700] loss: 0.099\n",
            "[3,   800] loss: 0.094\n",
            "[3,   900] loss: 0.095\n",
            "\n",
            "Training with Batch Normalization:\n",
            "[1,   100] loss: 0.464\n",
            "[1,   200] loss: 0.253\n",
            "[1,   300] loss: 0.219\n",
            "[1,   400] loss: 0.172\n",
            "[1,   500] loss: 0.183\n",
            "[1,   600] loss: 0.152\n",
            "[1,   700] loss: 0.161\n",
            "[1,   800] loss: 0.149\n",
            "[1,   900] loss: 0.143\n",
            "[2,   100] loss: 0.118\n",
            "[2,   200] loss: 0.106\n",
            "[2,   300] loss: 0.098\n",
            "[2,   400] loss: 0.102\n",
            "[2,   500] loss: 0.101\n",
            "[2,   600] loss: 0.105\n",
            "[2,   700] loss: 0.105\n",
            "[2,   800] loss: 0.102\n",
            "[2,   900] loss: 0.096\n",
            "[3,   100] loss: 0.076\n",
            "[3,   200] loss: 0.071\n",
            "[3,   300] loss: 0.076\n",
            "[3,   400] loss: 0.078\n",
            "[3,   500] loss: 0.070\n",
            "[3,   600] loss: 0.084\n",
            "[3,   700] loss: 0.083\n",
            "[3,   800] loss: 0.074\n",
            "[3,   900] loss: 0.084\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Transformations for the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Loading MNIST dataset\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Define a simple neural network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, use_batch_norm=False):\n",
        "        super(Net, self).__init__()\n",
        "        self.use_batch_norm = use_batch_norm\n",
        "\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "        # Batch Normalization layers\n",
        "        if use_batch_norm:\n",
        "            self.bn1 = nn.BatchNorm1d(512)\n",
        "            self.bn2 = nn.BatchNorm1d(256)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "\n",
        "        # Apply batch normalization if specified\n",
        "        if self.use_batch_norm:\n",
        "            x = self.bn1(x)\n",
        "\n",
        "        x = torch.relu(self.fc2(x))\n",
        "\n",
        "        # Apply batch normalization if specified\n",
        "        if self.use_batch_norm:\n",
        "            x = self.bn2(x)\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, criterion, optimizer, epochs=3):\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss / 100))\n",
        "                running_loss = 0.0\n",
        "\n",
        "# Training without batch normalization\n",
        "model_no_bn = Net(use_batch_norm=False)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_no_bn.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "print(\"Training without Batch Normalization:\")\n",
        "train_model(model_no_bn, criterion, optimizer)\n",
        "\n",
        "# Training with batch normalization\n",
        "model_with_bn = Net(use_batch_norm=True)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model_with_bn.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "print(\"\\nTraining with Batch Normalization:\")\n",
        "train_model(model_with_bn, criterion, optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformations for the MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Loading MNIST test dataset\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Function to calculate accuracy\n",
        "def get_accuracy(model, dataloader):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            inputs, labels = data\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return correct / total\n",
        "\n",
        "# Print accuracy for both models\n",
        "print(\"\\nAccuracy without Batch Normalization: {:.2%}\".format(get_accuracy(model_no_bn, testloader)))\n",
        "print(\"Accuracy with Batch Normalization: {:.2%}\".format(get_accuracy(model_with_bn, testloader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtuQiXpSgP1W",
        "outputId": "2b82f660-c700-45c4-e308-aa229bfe06e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accuracy without Batch Normalization: 96.77%\n",
            "Accuracy with Batch Normalization: 97.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slightly faster convergence with better performance"
      ],
      "metadata": {
        "id": "B755UWceicci"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mmu9GPC5gQPJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}